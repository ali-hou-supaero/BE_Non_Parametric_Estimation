---
title: 'Nonparametrics statistics: BE'
author: "Ali HOUSSENALY - Maël DACHER"
format: 
  html:
     embed-resources: true
  pdf: default
date: "October 10, 2025"
toc: true
toc-depth: 2
number-sections: true
number-depth: 2
editor_options: 
  chunk_output_type: console
execute:
  cache: true
  freeze: auto
---
```{r, eval=TRUE}
set.seed(42)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
QQ <- 0
```

### For instructions, see the page of the course: [https://lms.isae.fr/course/view.php?id=1014](https://lms.isae.fr/course/view.php?id=1014)


# Introduction

## Guidelines

- deadline: Oct 17, 2025 at 12:00 (noon)
- you must submit both the source (.qmd) and the compiled (.pdf or .html) document
- the source document must compile with no errors
- whenever possible, please relate your comments to the notions studied in class (overfitting, bias/variance tradeoff, regularity of the target function...)
- the presentation of the document is taken into account of the evaluation

## Setup

We focus on nonparametric kernel regression and consider the Mean Squared Error as a measure of risk. We consider the problem of estimating the function $f$ (supposed to be unknown):

```{r, dafun, echo=FALSE}
f <- readRDS("f.rds")
curve(f)
```


The function `sim_Y` (whose source code is hidden) allows to generate noisy samples from $f$:

```{r sim}
sim_Y <- readRDS("sim_Y.rds")
```


```{r}
n <- 500
X <- 1:n/n
Y <- sim_Y(X)
plot(X, Y, pch = 19, cex= 0.2)
```

## R functions for nonpametric regression

We will use the `R` function `locpoly` from the package `KernSmooth`. This function implements local polynomial estimators with a Gaussian kernel.

```{r libs}
library("KernSmooth")
```

We begin by looking at the help pages of this function:

```{r help}
# ?locpoly
```

The most important arguments for us are `x`, `y`, `bandwidth`.

In particular, the Nadaraya-Watson (NW) estimator corresponds to a local polynomial estimator of order 0. We will work with the following custom function, which outputs the NW estimates at the design points (see @sec-appendix for details):

```{r locpoly2}
fit_NW <- function(x, y, bandwidth, gridsize = length(x), ...) {
  locpoly(x, y, degree = 0, bandwidth = bandwidth, gridsize = gridsize, ...)
}
```


# Nadaraya-Watson estimator

We calculate the Nadaraya-Watson estimator and plot it along with the data points:

```{r, show=FALSE}
fit <- fit_NW(X, Y, bandwidth = 0.1)
plot(X, Y)
lines(fit, col=2)
lgd <- c("data points", "NW estimation")
legend("bottom", lgd, col = 1:2, lty = c(NA, 1), pch = c(1, NA))
```


```{r, echo=FALSE}
QQ <- QQ + 1
```

## Influence of the bandwidth {#sec-bandwidth}

::: {.callout-important collapse="false"}
### Question `r QQ`
Use the code below to compare graphically several choices for the bandwidth. Comment on the influence of the bandwidth on the quality of estimation. 
:::



::: {.callout-note collapse="false"}
### Answer `r QQ`

```{r}
## code to answer
par(lwd = 2)
plot(X, Y, col = "lightgray")
curve(f, add=TRUE)
bwd <- c(1, 0.1, 0.01)
for (kk in 1:length(bwd)) {
    fit <- fit_NW(X, Y, bandwidth = bwd[kk])
    lines(fit, col = 1 + kk)
}
lgd <- paste0("h = ", bwd)
legend("bottom", lgd, col = 2:4, lty = 1)
```

The bandwidth $h$ determines the width of the window around each point used to compute local averages or densities.

- Small bandwidth : estimator is highly sensitive to data, capturing fine details but also noise. This leads to low bias and high variance. This is the overfitting case.

- Large bandwidth : estimator smooths over more data, reducing noise but potentially missing important structure. This leads to high bias and low variance. This is the underfitting case.

This is the bias-variance tradeoff.

In the plot above, we show that $h = 1$ doesn't capture any detail because the bandwidth equals the whole x range.
On the other hand, $h = 0.01$ captures a lot more details of the points and tends to overfit the estimation. You can see below $h = 0.001$ largely overfits to the data because the density estimation is done on each point capturing all details.



```{r}
## code to answer
par(lwd = 2)
plot(X, Y, col = "lightgray")
curve(f, add=TRUE)
bwd <- c(1, 0.1, 0.001)
for (kk in 1:length(bwd)) {
    fit <- fit_NW(X, Y, bandwidth = bwd[kk])
    lines(fit, col = 1 + kk)
}
lgd <- paste0("h = ", bwd)
legend("bottom", lgd, col = 2:4, lty = 1)
```
:::
## Leave-one-out cross-validation, take 1

Let $\hat{f}_{h}$ be a generic non-parametric estimator of $f$ built from the sample $Y = (Y_1, \dots, Y_n)$. 
The leave-one-out (LOO) cross-validation risk of $\hat{f}_{h}$ is defined as

$$LOOCV(h) = \frac{1}{n} \sum_{i=1}^n \left(Y_i - \hat{f}^{-i}_{h}(X_i) \right)^2,$$ {#eq-LOOCV}

where $\hat{f}^{Y^{-i}}_{h}$ is the corresponding non-parametric estimator of $f$ built from the sample $Y^{-i} = (Y_1, \dots, Y_{i-1}, Y_{i+1}, \dots, Y_n)$.


```{r, echo=FALSE}
QQ <- QQ + 1
```

::: {.callout-important collapse="false"}
### Question `r QQ`
Write a function `LOOCV` based on `locpoly` to estimate the LOO cross-validation risk of the NW estimator. 
:::
  
::: {.callout-note collapse="false"}
### Answer `r QQ`
```{r, eval=TRUE}
LOOCV <- function(X, Y, bandwidths) {
  n <- length(X)
  MSE_LOO <- numeric(length(bandwidths))
  for (b in seq(along = bandwidths)) {
    bandwidth <- bandwidths[b]
    fit1 <- rep(NA, n)
    for (i in 1:n) {
      fit <- fit_NW(X[-i], Y[-i], bandwidth = bandwidth, gridsize=n)
      if (fit$x[i] == X[i]) {
        fit1[i] <- fit$y[i]
      }
    }
    MSE_LOO[b] <- mean((fit1 - Y)^2, na.rm = TRUE)
  }
  return(MSE_LOO)
}
```
:::


```{r, echo=FALSE}
QQ <- QQ + 1
```

::: {.callout-important collapse="false"}
### Question `r QQ`
Use the function `LOOCV` to plot the LOO risk as a function of the bandwidth. What is the optimal bandwidth?
:::

::: {.callout-note collapse="false"}
### Answer `r QQ`
```{r, eval=TRUE}
bandwidths <- seq(from = 0.005, to = 0.1, by = 0.005)
res <- LOOCV(X, Y, bandwidths)
plot(bandwidths, res, t= 'b', ylab = "LOO Risk", xlab = "Bandwidth")
```
:::
Based on the graph above we see that the optimal bandwidth is $\approx 0.03$ because it is the value that minimizes LOO risk.


```{r, echo=FALSE}
QQ <- QQ + 1
```

::: {.callout-important collapse="false"}
### Question `r QQ` (bonus question)
Implement a dichotomy search to find the optimal bandwith.
:::

::: {.callout-note collapse="false"}
### Answer `r QQ`
```{r, eval=TRUE}
dichotomy_search_LOOCV <- function(X, Y, lower = 0.005, upper = 0.1, tol = 1e-4, max_iter = 50) {
  iter <- 0
  
  while ((upper - lower) > tol && iter < max_iter) {
    iter <- iter + 1
    
    mid <- (lower + upper) / 2
    left <- (lower + mid) / 2
    right <- (mid + upper) / 2
    
    # Evaluate LOO risk
    risk_left <- LOOCV(X, Y, bandwidths = left)
    risk_mid  <- LOOCV(X, Y, bandwidths = mid)
    risk_right <- LOOCV(X, Y, bandwidths = right)
    
    # Update values depending on curve
    if (risk_left < risk_mid) {
      upper <- mid
    } else if (risk_right < risk_mid) {
      lower <- mid
    } else {
      lower <- left
      upper <- right
    }
  }
  
  # Optimal bandwidth
  return((lower + upper) / 2)
}

optimal_bw <- dichotomy_search_LOOCV(X, Y)
cat("Optimal bandwidth:", optimal_bw, "\n")

# Visualization
res <- LOOCV(X, Y, bandwidths)
plot(bandwidths, res, type = 'b', ylab = "LOO Risk", xlab = "Bandwidth")
abline(v = optimal_bw, col = "red", lty = 2)
```
We see that we obtain the same result as the one estimated before on the graph.
:::



```{r, echo=FALSE}
QQ <- QQ + 1
```

::: {.callout-important collapse="false"}
### Question `r QQ`
Plot the running time of `LOOCV` as a function of $n$. What it its time complexity? Can this estimator be run on large sample sizes, ie for $n \gg 1000$ ?
```{r, eval=TRUE}
n_values <- seq(from = 100, to = 2000, by = 100) 

times <- sapply(n_values, function(n) {
  X <- 1:n/n
  Y <- sim_Y(X)
  system.time(LOOCV(X, Y, optimal_bw))["elapsed"]
})
plot(n_values, times, t="b", xlab="Sample size (n)", ylab="Time (s)")
```
The time complexity of the LOOCV function looks polynomial. We won't be able to tu use this estimator on large sample sizes. We are not confronted here to the curse of dimensionality seen in class (related to the statistical complexity), since we are rising the number of samples. But the time complexity is quadratic. 

The time complexity of the LOOCV1 risk can be analytically obtained:

$$
LOOCV(h) = \frac{1}{n} \sum_{i=1}^n \left(Y_i - \hat{f}^{-i}_{h}(X_i) \right)^2
\text{with } \hat{f}^{-i}_{h}(x) = \hat{f}^{NW}_{h, -i}(x) = \frac{\sum_{j=1, j\neq i}^n \left( Y_j K(\frac{X_j-x}{h}) \right)}{\sum_{j=1, j\neq i}^n \left( K(\frac{X_j-x}{h}) \right)}
$$

We can see that there is approximately $n^2$ operations. So the time complexity is $O(n^2)$.
Depending on the computer, we were able to compute the risk with $n=2000$, but it was long.

:::
## LOOCV, take 2

The goal of this part is to speed up the computation of the NW estimator, by going back to its mathematical definition.

```{r, echo=FALSE}
QQ <- QQ + 1
```

::: {.callout-important collapse="false"}
### Question `r QQ`
Write a function `LOOCV2` *not based on `locpoly`* to estimate the LOO cross-validation risk of the NW estimator. Check that it produces the same results as `LOOCV1`. 
:::
  
::: {.callout-note collapse="false"}
### Answer `r QQ`
```{r, eval=TRUE}
# Gaussian kernel
gaussian_kernel <- function(u) {
  exp(-0.5 * u^2) / sqrt(2 * pi)
}


# Fast LOOCV using NW definition
LOOCV2 <- function(X, Y, bandwidths) {
  n <- length(X)
  MSE_LOO <- numeric(length(bandwidths))
  
  for (b in seq_along(bandwidths)) {
    h <- bandwidths[b]
    fit1 <- numeric(n)
    
    for (i in 1:n) {
      # Compute kernel weights excluding i
      x_i <- X[i]
      x_loo <- X[-i]
      y_loo <- Y[-i]
      
      weights <- gaussian_kernel((x_i - x_loo) / h)
      fit1[i] <- sum(weights * y_loo) / sum(weights)
    }
    
    MSE_LOO[b] <- mean((fit1 - Y)^2)
  }
  
  return(MSE_LOO)
}

res1 <- LOOCV(X, Y, bandwidths)
res2 <- LOOCV2(X, Y, bandwidths)

# Compare results
plot(bandwidths, res1, type = "b", col = "blue", ylim = range(c(res1, res2)),
     ylab = "LOO Risk", xlab = "Bandwidth", main = "LOOCV1 vs LOOCV2")
lines(bandwidths, res2, type = "b", col = "red", pch = 19)
legend("topright", legend = c("LOOCV1 (locpoly)", "LOOCV2 (fast)"),
       col = c("blue", "red"), lty = 1, pch = c(1, 19))
```
We observe almost identical plots for the 2 implementations. Since the optimal bandwidth is not affected by the minor differences in LOO Risk between the 2 curves, we can conclude that our implementation is correct.
:::

```{r, echo=FALSE}
QQ <- QQ + 1
```

::: {.callout-important collapse="false"}
### Question `r QQ`
Plot the running time of `LOOCV` and `LOOCV2` as a function of $n$. Can `LOOCV2` be run on large sample sizes?
:::

::: {.callout-note collapse="false"}
### Answer `r QQ`
```{r, eval=TRUE}
# Time measure for LOOCV (locpoly)
times_LOOCV <- sapply(n_values, function(n) {
  X <- 1:n/n
  Y <- sim_Y(X)
  system.time(LOOCV(X, Y, optimal_bw))["elapsed"]
})

# Time measure for LOOCV2 (fast version)
times_LOOCV2 <- sapply(n_values, function(n) {
  X <- 1:n/n
  Y <- sim_Y(X)
  system.time(LOOCV2(X, Y, optimal_bw))["elapsed"]
})

# Comparative plot
plot(n_values, times_LOOCV, type = "l", col = "red", lwd = 2,
     xlab = "Sample size (n)", ylab = "Time (s)",
     main = "Run time LOOCV vs LOOCV2", 
     ylim = range(times_LOOCV2)
     )
lines(n_values, times_LOOCV2, col = "blue", lwd = 2)
legend("topleft", legend = c("LOOCV (locpoly)", "LOOCV2 (fast)"),
       col = c("red", "blue"), lwd = 2)
```
Based on the previous question, we confirmed that the 2 implementations yield the same results. Therefore their time complexity can be compared.

The time complexity of LOOCV2 is better than LOOCV1. Its time complexity is obviously still quadratic like LOOCV1, but LOOCV2 has a better constant making it more suitable for computing large values of $n$.

At $n=2000$, $LOOCV1$ takes about 2s, whereas $LOOCV2$ takes 0.1s.

:::

## LOOCV, take 3

The Nadaraya-Watson estimator is an instance of linear nonparametric estimators, that is, it may be written as

$$ \hat{f}_{h}(x) = \sum_{i=1}^{n} Y_i W_{ni}(x, h), $$
where the weights $W_{ni}$ depend on the specific estimator. In the case of the Nadaraya-Watson estimator, we have

$$
W_{ni}^{NW}(x,h) = 
  \frac{K \left( \frac{X_i - x}{h} \right) }
       {\sum_{j=1}^n{K \left( \frac{X_j - x}{h} \right)}}
$$
A nice property of linear NP estimators is that the leave-one-out (LOO) cross-validation risk may be explicitly written as

$$LOOCV(h) = \frac{1}{n} \sum_{i=1}^n \left(\frac{Y_i - \hat{f}_h(X_i)}{1-W_{ni}(X_i, h)} \right)^2$$ {#eq-LOOCV-weights}


```{r, echo=FALSE}
QQ <- QQ +1
```

::: {.callout-important collapse="false"}
### Question `r QQ`
Using the definition of $LOOCV(h)$ (@eq-LOOCV), prove @eq-LOOCV-weights.
:::

::: {.callout-note collapse="false"}
### Answer `r QQ`
\noindent By definition, the leave-one-out (LOO) cross-validation risk is :
\[
LOOCV(h) = \frac{1}{n} \sum_{i=1}^n \left(Y_i - \hat{f}^{-i}_{h}(X_i) \right)^2
\]
Let's also define $\hat{f}^{-i}_h$ :
\[\hat{f}^{-i}_{h}(x) = \sum^n_{j=1, j\neq i}Y_j W_{nj}(x,h) = \frac{\sum^n_{j=1, j\neq i}Y_j K(\frac{X_j-x}{h})}{\sum^n_{j=1, j\neq i} K(\frac{X_j-x}{h})}\]
\[
\hat{f}^{-i}_{h}(x) = \frac{\sum^n_{j=1}Y_j K(\frac{X_j-x}{h}) - Y_i K(\frac{X_i-x}{h})}{\sum^n_{j=1} K(\frac{X_j-x}{h}) - K(\frac{X_i-x}{h})}
\]
\noindent
Let's note $S = \sum^n_{i=1}K(\frac{X_i-x}{h})$. Thus, we can write: 
\[
\hat{f}^{-i}_{h}(x) = \frac{S \cdot \hat{f}_{h}(x) - Y_i K(\frac{X_i-x}{h})}{S - K(\frac{X_i-x}{h})}
\]
\noindent From this points, if we isolate $\hat{f}_h$, we have:
\[
\hat{f}_h(x) = Y_i \cdot \frac{K(\frac{X_i - x}{h})}{S} + \hat{f}^{-i}_h(x)\cdot\frac{S-K(\frac{X_i - x}{h})}{S}
\]
\noindent But we can also write $\frac{K(\frac{X_i - x}{h})}{S}$ as $W_{ni}(x,h)$. So:
\[
\hat{f}_h(x) = Y_i \cdot W_{ni}(x,h) + \hat{f}^{-i}_h(x)\cdot(1-W_{ni}(x,h))
\]
\noindent Let's isolate $\hat{f}^{-i}_h$:
\[
\hat{f}^{-i}_h = \frac{\hat{f}_h - Y_i \cdot W_{ni}}{1-W_{ni}}
\]
\noindent We can now integrate this expression in the expression of the LOOCV risk:
\[
Y_i - \hat{f}^{-i}_{h}(X_i) = Y_i - \frac{\hat{f}_h(X_i) - Y_i \cdot W_{ni}(X_i,h)}{1-W_{ni}(X_i,h)} = \frac{Y_i - \hat{f}_h(X_i)}{1-W_{ni}(X_i,h)}
\]
\noindent This ends the proof.

:::

```{r, echo=FALSE}
QQ <- QQ +1
```

::: {.callout-important collapse="false"}
### Question `r QQ`
Write a function `LOOCV3` exploiting @eq-LOOCV-weights to estimate the LOO cross-validation risk of the NW estimator. Check that it produces the same results as `LOOCV1`. 
:::


::: {.callout-note collapse="false"}
### Answer `r QQ`
```{r, eval=TRUE}

LOOCV3 <- function(X, Y, bandwidths) {
  n <- length(X)
  MSE_LOO <- numeric(length(bandwidths))
  
  for (b in seq_along(bandwidths)) {
    h <- bandwidths[b]
    
    # Compute full NW estimator at each X[i]
    W <- matrix(0, n, n)
    for (i in 1:n) {
      for (j in 1:n) {
        W[i, j] <- gaussian_kernel((X[i] - X[j]) / h)
      }
    }
    
    # Normalize rows to get weights
    W_norm <- W / rowSums(W)
    
    # Compute fitted values
    f_hat <- rowSums(W_norm * matrix(Y, n, n, byrow = TRUE))
    
    # Extract diagonal weights W_ni(X_i, h)
    W_diag <- diag(W_norm)
    
    # Apply Equation (2)
    residuals <- (Y - f_hat) / (1 - W_diag)
    MSE_LOO[b] <- mean(residuals^2)
  }
  
  return(MSE_LOO)
}

# Compute LOO risks
res1 <- LOOCV(X, Y, bandwidths)     
res2 <- LOOCV2(X, Y, bandwidths)    
res3 <- LOOCV3(X, Y, bandwidths)    

# Plot
plot(bandwidths, res1, type = "b", col = "blue", ylim = range(c(res1, res2, res3)),
     ylab = "LOO Risk", xlab = "Bandwidth", main = "LOOCV1 vs LOOCV2 vs LOOCV3")

lines(bandwidths, res2, type = "b", col = "red", pch = 17)
lines(bandwidths, res3, type = "b", col = "darkgreen", pch = 19)

legend("topright",
       legend = c("LOOCV1 (locpoly)", "LOOCV2 (kernel)", "LOOCV3 (weights)"),
       col = c("blue", "red", "darkgreen"), lty = 1, pch = c(1, 17, 19))
```
The graph indicates that the three implementations yield similar results. LOOCV2 and LOOCV3 match perfectly. However, a small discrepancy remains with the LOOCV1 implementation. Since this difference is minimal and does not affect the value of the optimal bandwidth, we consider the three implementations equivalent in terms of accuracy. Therefore, we can focus on comparing their computational efficiency
:::

```{r, echo=FALSE}
QQ <- QQ +1
```


::: {.callout-important collapse="false"}
### Question `r QQ`
Plot the running time of `LOOCV` and `LOOCV2` as a function of $n$. Can `LOOCV2` be run on large sample sizes?
:::

::: {.callout-note collapse="false"}

### Answer `r QQ`

```{r, eval=TRUE}
# Time measure for LOOCV (locpoly)
times_LOOCV <- sapply(n_values, function(n) {
  X <- 1:n / n
  Y <- sim_Y(X)
  system.time(LOOCV(X, Y, optimal_bw))["elapsed"]
})

# Time measure for LOOCV2 (fast kernel-based)
times_LOOCV2 <- sapply(n_values, function(n) {
  X <- 1:n / n
  Y <- sim_Y(X)
  system.time(LOOCV2(X, Y, optimal_bw))["elapsed"]
})

# Time measure for LOOCV3 (weight-based using Equation 2)
times_LOOCV3 <- sapply(n_values, function(n) {
  X <- 1:n / n
  Y <- sim_Y(X)
  system.time(LOOCV3(X, Y, optimal_bw))["elapsed"]
})

# Comparative plot
plot(n_values, times_LOOCV, type = "b", col = "red", lwd = 2,
     xlab = "Sample size (n)", ylab = "Time (s)",
     main = "Run time LOOCV1 vs LOOCV2 vs LOOCV3")
lines(n_values, times_LOOCV2, col = "blue", lwd = 2, type="b")
lines(n_values, times_LOOCV3, col = "darkgreen", lwd = 2, type="b")
legend("topleft", legend = c("LOOCV (locpoly)", "LOOCV2 (kernel)", "LOOCV3 (weights)"),
       col = c("red", "blue", "darkgreen"), lwd = 2)
```

We observe that LOOCV3 is a bit better than LOOCV1 however, both appear to follow the same polynomial trend. 
Analytically, the formula of the LOOCV3 indicates that its time complexity is still $O(n)$.
It is clear from the plot that the LOOCV2 implementation performs best for large values of $n$.
:::


# Local polynomial estimators of order 1

## Limitations of the NW estimator

```{r, echo=FALSE}
QQ <- QQ +1
```

::: {.callout-important collapse="false"}
### Question `r QQ`
Plot the NW estimator obtained by LOOCV. Can you explain why it does not work uniformly well for all values of $x$? Compare to a local polynomial estimator of order 1 ("LP1 estimator")
:::

::: {.callout-note collapse="false"}
### Answer `r QQ`


```{r, show=TRUE}
fit_NW_result <- fit_NW(X, Y, optimal_bw)

# LP1 estimator (local polynomial of degree 1)
fit_LP1 <- locpoly(X, Y, degree = 1, bandwidth=optimal_bw, gridsize = length(X))

# Plot data and both estimators
plot(X, Y, main = "NW vs LP1 Estimator", xlab = "X", ylab = "Y", pch = 1, col="green")
lines(fit_NW_result, col = "blue", lwd = 2)
lines(fit_LP1, col = "red", lwd = 2)

# Legend
legend("bottom", legend = c("Data points", "NW estimation", "LP1 estimation"),
       col = c("green", "blue", "red"), lty = c(NA, 1, 1), pch = c(1, NA, NA))
```
The Nadaraya-Watson (NW) estimator obtained using the optimal bandwidth from LOOCV fits the data well across most of the domain. However, it does not perform uniformly well for all values of $x$. In particular, near the boundaries (extreme values of x), the NW estimator tends to show increased bias. This is because it is a local constant estimator: it averages nearby values without accounting for local trends or slopes.

In contrast, the local polynomial estimator of order 1 (LP1) fits a local linear model, which allows it to extrapolate more effectively near boundaries and adapt to local slopes. As seen in the graph, LP1 and NW produce nearly identical fits in the central region, but LP1 performs slightly better at the extremes, where NW tends to flatten or overshoot.

:::

## LOOCV, take 1

We now tackle the problem of LOO estimation of the best bandwidth for the LP1 estimator.

```{r, echo=FALSE}
QQ <- QQ + 1
```

::: {.callout-important collapse="false"}
### Question `r QQ`
Write a function `LOOCV_LP1` based on `locpoly` to estimate the LOO cross-validation risk of the LP1 estimator. 
:::


::: {.callout-note collapse="false"}
### Answer `r QQ`
```{r, show=TRUE}
LOOCV_LP1 <- function(X, Y, bandwidths) {
  n <- length(X)
  MSE_LOO <- numeric(length(bandwidths))
  
  for (b in seq_along(bandwidths)) {
    bandwidth <- bandwidths[b]
    fit1 <- rep(NA, n)
    
    for (i in 1:n) {
      # Leave-one-out data
      X_loo <- X[-i]
      Y_loo <- Y[-i]
      
      # Fit LP1 estimator on reduced data
      fit <- locpoly(X_loo, Y_loo, degree = 1, bandwidth = bandwidth, gridsize = n)
      
      idx <- which.min(abs(fit$x - X[i]))
      fit1[i] <- fit$y[idx]
    }
    
    MSE_LOO[b] <- mean((fit1 - Y)^2, na.rm = TRUE)
  }
  
  return(MSE_LOO)
}

res_lp1 <- LOOCV_LP1(X, Y, bandwidths)

# Compare with NW-based LOOCV
res_nw <- LOOCV(X, Y, bandwidths)

plot(bandwidths, res_nw, type = "b", col = "blue", ylim = range(c(res_nw, res_lp1)),
     ylab = "LOO Risk", xlab = "Bandwidth", main = "LOOCV: NW vs LP1")
lines(bandwidths, res_lp1, type = "b", col = "red", pch = 17)
legend("topright", legend = c("NW (degree 0)", "LP1 (degree 1)"),
       col = c("blue", "red"), lty = 1, pch = c(1, 17))
```
:::

## LOOCV, take 2

Prove that the LP1 estimator may be written as:

$$
\hat{f}^{LP1}(x) 
= \frac{S_{2}(x)\,S_{0}'(x) \;-\; S_{1}(x)\,S_{1}'(x)}
       {S_{2}(x)\,S_{0}(x) \;-\; S_{1}^2(x)},
$${#eq-LP1}

where for $k=0, 1, 2$, we have

$$
S_k(x) = \sum_{i=1}^n (x_i - x)^k\, K\left(\tfrac{x_i - x}{h}\right)
$$
and

$$
S_{k}'(x) 
\;=\; \sum_{i=1}^n (x_i - x)^k \, y_i \, K\!\left(\tfrac{x_i - x}{h}\right).
$$

```{r, echo=FALSE}
QQ <- QQ + 1
```

::: {.callout-note collapse="false"}
### Answer `r QQ`

Thanks to the class, we know that the local polynomial estimator of order $1$ of the regression with bandwidth $h$ associated to the kernel $K$ is the first coordinate of $\hat{\theta}_n$, with $\hat{\theta}_n$ such that:
\[
\hat{\theta}_n(x) = arg \min_{\theta \in \mathbb{R}^2} \sum^n_{i=1}\left( Y_i -  \theta^\top \Pi_1(\frac{X_i -x}{h})\right)^2 K\left(\frac{X_i-x}{h}\right)
\]
\noindent with $\Pi_1(x) = a_0 + a_1\cdot x$. And we can redefine this problem as a weighted least square problem with the solution:
\[
\hat{\beta} = (X^\top W X)^{-1} X^\top W Y
\]

\noindent with:
\[
X = 
\begin{bmatrix}
    1 & \frac{X_1 - x}{h} \\
    1 & \frac{X_2 - x}{h}\\
    \vdots & \vdots \\
    1 & \frac{X_n - x}{h}
\end{bmatrix}
\]
\[
W = \text{diag}\left( K\left( \frac{X_1 - x}{h} \right), \dots, K\left( \frac{X_n - x}{h} \right) \right)
\]
\[
Y = \left[Y_1, Y_2, \dots, Y_n \right]^\top
\]

\noindent Now, thanks to the definitions of $S_k$ for $k \in [0,1,2]$, we have:

\[
X^\top W X =
\begin{bmatrix}
S_0(x) & S_1(x) \\
S_1(x) & S_2(x)
\end{bmatrix}, \quad
X^\top W Y =
\begin{bmatrix}
S_0'(x) \\
S_1'(x)
\end{bmatrix}
\]
\noindent We can inverse this 2x2 matrix:

\[
(X^\top W X)^{-1} =
\frac{1}{S_0(x)S_2(x) - S_1(x)^2}
\begin{bmatrix}
S_2(x) & -S_1(x) \\
-S_1(x) & S_0(x)
\end{bmatrix}
\]


\noindent Now, we inject into $\beta$:

\[
\hat{\beta} =
\frac{1}{S_0 S_2 - S_1^2}
\begin{bmatrix}
S_2 & -S_1 \\
-S_1 & S_0
\end{bmatrix}
\begin{bmatrix}
S_0' \\
S_1'
\end{bmatrix}
=
\frac{1}{S_0 S_2 - S_1^2}
\begin{bmatrix}
S_2 S_0' - S_1 S_1' \\
\cdots
\end{bmatrix}
\]


And the LP1 estimator is the intercept $\hat{f}^{LP1}(x) = \hat{\beta}_0$, so:

\[
\hat{f}^{LP1}(x) = \frac{S_2(x) S_0'(x) - S_1(x) S_1'(x)}{S_2(x) S_0(x) - S_1(x)^2}
\]

:::

::: {.callout-important collapse="false"}
### Question `r QQ`
Using @eq-LP1, write a function `LOOCV2_LP1` based on `locpoly` to estimate the LOO cross-validation risk of the LP1 estimator. Check that it produces the same results as `LOOCV_LP1` and compare their speed.  
:::

::: {.callout-note collapse="false"}
### Answer `r QQ`

```{r, show=TRUE}

# LOOCV2_LP1 function
LOOCV2_LP1 <- function(X, Y, bandwidths) {
  n <- length(X)
  MSE_LOO <- numeric(length(bandwidths))
  
  for (b in seq_along(bandwidths)) {
    h <- bandwidths[b]
    
    # Precompute all kernel weights
    Kmat <- outer(X, X, function(xi, xj) dnorm((xj - xi) / h))
    
    # Compute weighted sums S_k(x) and S_k'(x)
    S0 <- rowSums(Kmat)
    S1 <- rowSums((X - rep(X, each = n)) * Kmat)
    S2 <- rowSums((X - rep(X, each = n))^2 * Kmat)
    
    S0p <- rowSums(Kmat * matrix(Y, n, n, byrow = TRUE))
    S1p <- rowSums((X - rep(X, each = n)) * Kmat * matrix(Y, n, n, byrow = TRUE))
    
    # Compute fitted values using LP1 formula
    denom <- S2 * S0 - S1^2
    f_hat <- (S2 * S0p - S1 * S1p) / denom
    
    # Compute diagonal weights W_ii for correction
    W_diag <- dnorm(0) / S0
    
    # Apply LOO correction
    residuals <- (Y - f_hat) / (1 - W_diag)
    MSE_LOO[b] <- mean(residuals^2)
  }
  
  return(MSE_LOO)
}

# LOO Risk plot
res_lp1_slow <- LOOCV_LP1(X, Y, bandwidths)
res_lp1_fast <- LOOCV2_LP1(X, Y, bandwidths)


plot(bandwidths, res_lp1_slow, type = "b", col = "blue", ylim = range(c(res_lp1_slow, res_lp1_fast)),
     ylab = "LOO Risk", xlab = "Bandwidth", main = "LOOCV_LP1 vs LOOCV2_LP1")
lines(bandwidths, res_lp1_fast, type = "b", col = "darkgreen", pch = 19)
legend("topright", legend = c("LOOCV_LP1 (locpoly)", "LOOCV2_LP1 (equation (3))"),
       col = c("blue", "darkgreen"), lty = 1, pch = c(1, 19))


```
The 2 implementations show really close LOO Risk curves. This confirms that the 2 implementations provide similar results. We can therefore compare their time complexity.

```{r, show=TRUE}

# Optimal bandwidth value based on the graph
optimal_bw_L1 <- which.min(res_lp1_slow)
cat("Optimal bandwidth for LOOCV1_LP1:", optimal_bw_L1, "\n")

# Runtime comparison
# Measure time for LOOCV_LP1
times_lp1_slow <- sapply(n_values, function(n) {
  X <- 1:n / n
  Y <- sim_Y(X)
  system.time(LOOCV_LP1(X, Y, optimal_bw_L1))["elapsed"]
})

# Measure time for LOOCV2_LP1
times_lp1_fast <- sapply(n_values, function(n) {
  X <- 1:n / n
  Y <- sim_Y(X)
  system.time(LOOCV2_LP1(X, Y, optimal_bw_L1))["elapsed"]
})

# Plot runtime comparison
plot(n_values, times_lp1_slow, type = "b", col = "blue", lwd = 2,
     xlab = "Sample size (n)", ylab = "Time (s)",
     main = "Runtime: LOOCV_LP1 vs LOOCV2_LP1")
lines(n_values, times_lp1_fast, type = "b", col = "darkgreen", lwd = 2, pch = 19)
legend("topleft", legend = c("LOOCV_LP1 (locpoly)", "LOOCV2_LP1 (equation (3))"),
       col = c("blue", "darkgreen"), lwd = 2, pch = c(1, 19))

```
We can see from this runtime comparison that LOOCV1_LP1 is indeed slower than LOOCV2_LP2. The implementation based on @eq-LP1 is better.
:::

## LOOCV, take 3

Since local polynomial estimators are also linear nonparametric estimators in the sense of @eq-LOOCV-weights, their LOO risk may also be written as in @eq-LOOCV-weights.  

The matrix of weights $W=(W_{ni}(X_j))_{i,j}$ for local polynomials may be obtained using the following `R` function

```{r}
getW <- function(X, h) {
    n <- length(X)
    W <- matrix(NA, n, n)
    for (ii in 1:n){
        Yi <- rep(0, n)
        Yi[ii] <- 1
        fit <- locpoly(X, Yi, bandwidth = h, gridsize = n)
        W[ii, ] <- fit$y
    }
    W
}
```

```{r, echo=FALSE}
QQ <- QQ +1
```

::: {.callout-important collapse="false"}
### Question `r QQ`
Explain why the above code does indeed calculate the weights $W$.
:::
  
::: {.callout-note collapse="false"}
### Answer `r QQ`
The function `getW` computes the weight matrix $W = (W_{ni}(X_j))_{i,j}$ for the local polynomial estimator by exploiting its linearity in the response vector $Y$.

Since local polynomial estimators are linear nonparametric estimators, the fitted value at point $X_j$ can be written as:

$$
\hat{f}(X_j) = \sum_{i=1}^n W_{ni}(X_j) \, y_i
$$

To extract the weights $W_{ni}(X_j)$, we evaluate the estimator at each $X_j$ using a vector $Y_i$ that is zero everywhere except at position $i$, where it equals 1. This isolates the contribution of $y_i$ to the fitted value at each location.

In the code:

- For each $i = 1, \dots, n$, we construct a vector $Y_i$ with a 1 at position $i$ and 0 elsewhere.
- We then apply `locpoly(X, Y_i, ...)`, which returns the fitted values at all grid points (here chosen to match $X$).
- The resulting vector `fit$y` contains the weights $W_{ni}(X_j)$ for fixed $i$ and varying $j$.
- By repeating this for all $i$, we fill the matrix $W$ row by row.

Thus, `W[ii, j]` stores the weight assigned to $y_{ii}$ when estimating $\hat{f}(X_j)$, which matches the definition of $W_{ni}(X_j)$.
:::


```{r, echo=FALSE}
QQ <- QQ +1
```

::: {.callout-important collapse="false"}
### Question `r QQ`
Write a function `LOOCV2_LP1` to estimate the LOO cross-validation risk for a local polynomial estimator of order 1.
:::
  
::: {.callout-note collapse="false"}
### Answer `r QQ`
```{r, eval=TRUE}
LOOCV3_LP1 <- function(X, Y, bandwidths) {
  n <- length(X)
  MSE_LOO <- numeric(length(bandwidths))
  
  for (b in seq_along(bandwidths)) {
    h <- bandwidths[b]
    
    # Get weight matrix: W[i, j] = W_{ni}(X_j)
    W <- getW(X, h)
    
    # Each column j gives weights for X_j → transpose to align with Y
    f_hat <- t(W) %*% Y
    
    # Diagonal weights W_ii = contribution of y_i to f(X_i)
    W_diag <- diag(t(W))
    
    # LOO correction
    residuals <- (Y - f_hat) / (1 - W_diag)
    MSE_LOO[b] <- mean(residuals^2)
  }
  
  return(MSE_LOO)
}

# Compute LOO risks
res1 <- LOOCV_LP1(X, Y, bandwidths)      
res2 <- LOOCV2_LP1(X, Y, bandwidths)     
res3 <- LOOCV3_LP1(X, Y, bandwidths)    

# Plot comparison
plot(bandwidths, res1, type = "b", col = "blue", ylim = range(c(res1, res2, res3)),
     ylab = "LOO Risk", xlab = "Bandwidth", main = "LOO Risk: LP1 Estimators")
lines(bandwidths, res2, type = "b", col = "red", pch = 17)
lines(bandwidths, res3, type = "b", col = "darkgreen", pch = 19)
legend("topright", legend = c("LOOCV1_LP1 (locpoly)", "LOOCV2_LP1 (equation (3))", "LOOCV3_LP1 (getW)"),
       col = c("blue", "red", "darkgreen"), lty = 1, pch = c(1, 17, 19))
```
Our 3 implementations yield the same results. We can compare their time complexity.
:::

```{r, echo=FALSE}
QQ <- QQ +1
```


::: {.callout-important collapse="false"}
### Question `r QQ`
What is the time complexity of this computation as a function of $n$? Compare numerically the computing time of `LOOCV1_LP1` and `LOOCV2_LP1` for several values of $n$ and comment on the results.
:::
  
::: {.callout-note collapse="false"}
### Answer `r QQ`

```{r, eval=TRUE}

# Runtime comparison
# Measure time for LOOCV_LP1
times_lp1_1 <- sapply(n_values, function(n) {
  X <- 1:n / n
  Y <- sim_Y(X)
  system.time(LOOCV_LP1(X, Y, optimal_bw_L1))["elapsed"]
})

# Measure time for LOOCV2_LP1
times_lp1_2 <- sapply(n_values, function(n) {
  X <- 1:n / n
  Y <- sim_Y(X)
  system.time(LOOCV2_LP1(X, Y, optimal_bw_L1))["elapsed"]
})

# Measure time for LOOCV3_LP1
times_lp1_3 <- sapply(n_values, function(n) {
  X <- 1:n / n
  Y <- sim_Y(X)
  system.time(LOOCV3_LP1(X, Y, optimal_bw_L1))["elapsed"]
})


# Plot runtime comparison
plot(n_values, times_lp1_1, type = "b", col = "blue", lwd = 2,
     xlab = "Sample size (n)", ylab = "Time (s)",
     main = "Runtime: LOOCV_LP1 vs LOOCV2_LP1 vs LOOCV3_LP1")
lines(n_values, times_lp1_2, type = "b", col = "red", lwd = 2, pch = 17)
lines(n_values, times_lp1_3, type = "b", col = "darkgreen", lwd = 2, pch = 19)
legend("topleft", legend = c("LOOCV_LP1 (locpoly)", "LOOCV2_LP1 (equation (3))", "LOOCV3_LP1 (getW)"),
       col = c("blue", "red", "darkgreen"), lwd = 2, pch = c(1, 17, 19))
```
Similar to our previous findings with the NW estimator, the second implementation — based on a reformulation of the original LOO equation for the LP1 estimator — proves to be the most effective. The LOOCV2_LP1 method has better computational efficiency.
:::

## K-fold cross-validation

::: {.callout-important collapse="false"}
### Question `r QQ`
Give a formula for the K-fold cross-validation estimator of the MISE. 
:::
  
::: {.callout-note collapse="false"}
### Answer `r QQ`
The $K$-fold cross-validation estimator of the Mean Integrated Squared Error (MISE) is given by:

$$
\widehat{\text{MISE}}_{\text{CV}}(h) = \frac{1}{K} \sum_{k=1}^K \frac{1}{|I_k|} \sum_{i \in I_k} \left( y_i - \hat{f}_h^{(-k)}(x_i) \right)^2
$$

where:
- $I_1, \dots, I_K$ form a partition of $\{1, \dots, n\}$ into $K$ disjoint folds,
- $\hat{f}_h^{(-k)}$ is the estimator trained without the $k$-th fold,
- and $|I_k|$ is the number of observations in fold $k$.

This estimator approximates the MISE by averaging the squared prediction errors on held-out data across all folds.

:::


```{r, echo=FALSE}
QQ <- QQ +1
```

::: {.callout-important collapse="false"}
### Question `r QQ`
Implement this formula and compare numerically the results to those obtained by  `LOOCV1_LP1` or `LOOCV2_LP1`, both in terms of computation time and in terms of optimal bandwidth. Would you recommend $K$-fold CV or LOO for practical use?
:::
  
::: {.callout-note collapse="false"}
### Answer `r QQ`
```{r, eval=TRUE}

KFoldCV_LP1 <- function(X, Y, bandwidths, K = 5) {
  n <- length(X)
  folds <- split(sample(1:n), rep(1:K, length.out = n))
  MSE_KCV <- numeric(length(bandwidths))
  
  for (b in seq_along(bandwidths)) {
    h <- bandwidths[b]
    mse_fold <- numeric(K)
    
    for (k in 1:K) {
      idx_val <- folds[[k]]
      idx_train <- setdiff(1:n, idx_val)
      
      X_train <- X[idx_train]
      Y_train <- Y[idx_train]
      
      # Fit LP1 on training set
      fit <- locpoly(X_train, Y_train, degree = 1, bandwidth = h, gridsize = 100, range.x = range(X[idx_val]))
      
      # Interpolate predictions at validation points
      preds <- approx(fit$x, fit$y, xout = X[idx_val], rule = 2)$y
      
      mse_fold[k] <- mean((Y[idx_val] - preds)^2)
    }
    
    MSE_KCV[b] <- mean(mse_fold)
  }
  
  return(MSE_KCV)
}

# LOO Risk plot
risk_loocv1 <- LOOCV_LP1(X, Y, bandwidths)
risk_loocv2 <- LOOCV2_LP1(X, Y, bandwidths)
risk_kcv <- KFoldCV_LP1(X, Y, bandwidths, K = 5)



# Plot comparison
plot(bandwidths, risk_loocv1, type = "b", col = "blue", ylim = range(c(risk_loocv1, risk_loocv2, risk_kcv)),
     ylab = "CV Risk", xlab = "Bandwidth", main = "LOOCV vs K-Fold CV (LP1)")
lines(bandwidths, risk_loocv2, type = "b", col = "red", pch = 17)
lines(bandwidths, risk_kcv, type = "b", col = "darkgreen", pch = 19)
legend("topright", legend = c("LOOCV_LP1", "LOOCV2_LP1", "5-Fold CV"),
       col = c("blue", "red", "darkgreen"), lty = 1, pch = c(1, 17, 19))
```
Similar risk curves are obtained although we see slight differences in LOO risk and in optimal bandwidth values.
```{r, eval=TRUE}
# Runtime comparison
# Measure time for LOOCV_LP1
times_loocv1 <- sapply(n_values, function(n) {
  X <- 1:n / n
  Y <- sim_Y(X)
  system.time(LOOCV_LP1(X, Y, optimal_bw_L1))["elapsed"]
})

# Measure time for LOOCV2_LP1
times_loocv2 <- sapply(n_values, function(n) {
  X <- 1:n / n
  Y <- sim_Y(X)
  system.time(LOOCV2_LP1(X, Y, optimal_bw_L1))["elapsed"]
})

# Measure time for K fold
times_kfold <- sapply(n_values, function(n) {
  X <- 1:n / n
  Y <- sim_Y(X)
  system.time(KFoldCV_LP1(X, Y, optimal_bw_L1, K = 5))["elapsed"]
})


# Plot runtime comparison
plot(n_values, times_loocv1, type = "b", col = "blue", lwd = 2,
     xlab = "Sample size (n)", ylab = "Time (s)",
     main = "Runtime: LOOCV vs K-Fold CV (LP1)")
lines(n_values, times_loocv2, type = "b", col = "red", lwd = 2, pch = 17)
lines(n_values, times_kfold, type = "b", col = "darkgreen", lwd = 2, pch = 19)

legend("topleft", legend = c("LOOCV1_LP1", "LOOCV2_LP1", "5-Fold CV"),
       col = c("blue", "red", "darkgreen"), lwd = 2, pch = c(1, 17, 19))


```
With the example of 5-fold cross-validation, we see that computation time is significantly shorter than with LOOCV2_LP1. This makes it a more practical method for selecting the optimal bandwidth in real-world contexts, where sample sizes can become much larger than those simulated here.
:::

# Appendix {#sec-appendix}

## Why fixing the grid size in `locpoly`?

By default the `locpoly` function returns a vector of values for $\hat{f}(x)$ for 401 equally-spaced values of $x$ in $[0,1]$. Here, we want $\hat{f}(X_i)$ for $1 \leq i \leq n$,  where the $X_i$ are `n` equally-spaced values in $[0,1]$: 

```{r, eval=TRUE}
fit <- locpoly(X, Y, bandwidth = 1)
c(length(fit$y), length(X))
```

Our custom function  `fit_NW` forces the option `gridsize`  n `locpoly` to match the size of the input design: 

```{r, eval=TRUE}
fit <- fit_NW(X, Y, bandwidth = 1)
c(length(fit$y), length(X))
max(abs(X - fit$x))
```

